{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple audio denoiser using DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1y2pNZMM2aCL2WW3ZXvVd0_9FDwwXaE0G",
      "authorship_tag": "ABX9TyOUoQqBCZ4rhGHtRczqKWyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bibhas-Dash/Voice-Enhancement-using-Deep-Learning/blob/main/Simple_audio_denoiser_using_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U10eikaEZ2lh",
        "outputId": "20a28a6e-e57d-44c2-99b9-45cb105d785c"
      },
      "source": [
        "cd drive/My Drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeHFRclAYb6D"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "import librosa\r\n",
        "from sklearn import preprocessing as pp\r\n",
        "%matplotlib notebook"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7x7_v7sY08Y"
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\r\n",
        "config = tf.compat.v1.ConfigProto()\r\n",
        "config.gpu_options.allow_growth = True\r\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.33"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2NQO1qwk_SM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo0ag0dYY3M2"
      },
      "source": [
        "tf.compat.v1.set_random_seed(123)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sctaTt_PG1LM"
      },
      "source": [
        "n_file=30\r\n",
        "\r\n",
        "max_length = 60\r\n",
        "\r\n",
        "sr=8000\r\n",
        "\r\n",
        "window_size = 1024\r\n",
        "\r\n",
        "hop_length = 512"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5aamyzkAp0Y"
      },
      "source": [
        "def loadfile(path, str_tr, flag = 0):\r\n",
        "    list_tr = []\r\n",
        "    list_stft = []\r\n",
        "    list_stft_abs = []\r\n",
        "    list_length = []\r\n",
        "    z = ['0', '']\r\n",
        "    \r\n",
        "    for i in range(1,n_file+1):\r\n",
        "        if (i == 0):\r\n",
        "            j = 0\r\n",
        "        else:\r\n",
        "            j = int(math.log10(i))\r\n",
        "        s, sr = librosa.load(path + 'sp' + z[j] + str(i) + str_tr + '.wav', sr = None)\r\n",
        "        if (flag == 1):\r\n",
        "            list_tr.append(s)\r\n",
        "\r\n",
        "        #Calculating STFT\r\n",
        "        window = np.hanning(window_size)\r\n",
        "        \r\n",
        "        stft  = librosa.core.spectrum.stft(s, n_fft = window_size, hop_length = hop_length, window=window)\r\n",
        "        \r\n",
        "        stft_len = stft.shape[1]     \r\n",
        "\r\n",
        "        #Normalizing the data for training\r\n",
        "        if(flag==0):\r\n",
        "          stft = 2 * np.abs(stft) / np.sum(window)\r\n",
        "\r\n",
        "        #Appending STFT to list\r\n",
        "        if (flag == 1):\r\n",
        "            list_stft.append(stft)\r\n",
        "        \r\n",
        "        #Calculating Absolute of STFT\r\n",
        "        stft_abs = np.abs(stft)\r\n",
        "        \r\n",
        "        #Padding zeros to make length 60\r\n",
        "        stft_abs = np.pad(stft_abs, ((0,0),(0, max_length-stft_len)), 'constant')\r\n",
        "\r\n",
        "        #stft_abs =pp.normalize([stft_abs])\r\n",
        "        \r\n",
        "        #Appending abs to list\r\n",
        "        list_stft_abs.append(stft_abs)\r\n",
        "        \r\n",
        "        #Appending time-length of STFT to list\r\n",
        "        list_length.append(stft_len)\r\n",
        "        \r\n",
        "    return list_tr, list_stft, list_stft_abs, list_length"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLc4SFs7Y5MA"
      },
      "source": [
        "# Import train input and output data\r\n",
        "\r\n",
        "path_cl='/content/drive/MyDrive/Audio Enh. Dataset/clean_noizeus/wav/'\r\n",
        "path_ns='/content/drive/MyDrive/Audio Enh. Dataset/airport_10dB/wav/'"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2XNeuajLRQE"
      },
      "source": [
        "#Loading all training noisy speech signals\r\n",
        "\r\n",
        "trx, X, X_abs, X_len = loadfile(path_ns, '_airport_sn10')\r\n",
        "\r\n",
        "#Loading all training clean speech signals\r\n",
        "\r\n",
        "trs, S, S_abs, S_len = loadfile(path_cl, '')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZzPpYP6ZD5p"
      },
      "source": [
        "# Define fully connected network structure\r\n",
        "num_hidden_1 = 1000 # hidden layer 1\r\n",
        "num_hidden_2 = 1000 # hidden layer 2\r\n",
        "num_hidden_3 = 1000 # hidden layer 3\r\n",
        "num_hidden_4 = 1000 # hidden layer 4\r\n",
        "num_hidden_5 = 1000 # hidden layer 5\r\n",
        "num_features = 513 # input features\r\n",
        "num_output = 513 # number of outputs"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkBxLvDnZD8Q"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\r\n",
        "seq_len = tf.compat.v1.placeholder(tf.int32, None)\r\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, max_length, num_features])\r\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, max_length, num_output])\r\n",
        "keep_probability = tf.compat.v1.placeholder(\"float\")  # probability for dropouts"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR4aF4q1ZEBX"
      },
      "source": [
        "def fully_connected_net(data, keep_probability):\r\n",
        "    # Weights and bias initialization\r\n",
        "    weights = {\r\n",
        "        'w1': tf.Variable(tf.compat.v1.random_normal(shape=[num_features, num_hidden_1],stddev= tf.sqrt(2/(num_features+num_hidden_1)))),\r\n",
        "        'w2': tf.Variable(tf.compat.v1.random_normal(shape=[num_hidden_1, num_hidden_2],stddev= tf.sqrt(2/(num_hidden_1+num_hidden_2)))),\r\n",
        "        'w3': tf.Variable(tf.compat.v1.random_normal(shape=[num_hidden_2, num_hidden_3],stddev= tf.sqrt(2/(num_hidden_2+num_hidden_3)))),\r\n",
        "        'w4': tf.Variable(tf.compat.v1.random_normal(shape=[num_hidden_3, num_hidden_4],stddev= tf.sqrt(2/(num_hidden_3+num_hidden_4)))),\r\n",
        "        'w5': tf.Variable(tf.compat.v1.random_normal(shape=[num_hidden_4, num_hidden_5],stddev= tf.sqrt(2/(num_hidden_4+num_hidden_5)))),\r\n",
        "        'wout': tf.Variable(tf.compat.v1.random_normal(shape=[num_hidden_5, num_output],stddev = tf.sqrt(2/(num_hidden_5 + num_output))))\r\n",
        "    }\r\n",
        "    biases = {\r\n",
        "        'bout': tf.Variable(tf.compat.v1.random_normal([num_output]))\r\n",
        "    }\r\n",
        "    # Layer 1 with batch normalization\r\n",
        "    z1 = data@weights['w1']\r\n",
        "    batch_mean1, batch_var1 = tf.nn.moments(z1, [0])\r\n",
        "    z1hat = (z1 - batch_mean1) / tf.sqrt(batch_var1+epsilon)\r\n",
        "    # Create two new parameters, scale and beta (shift)\r\n",
        "    scale1 = tf.Variable(tf.ones([num_hidden_1]))\r\n",
        "    beta1 = tf.Variable(tf.zeros([num_hidden_1]))\r\n",
        "    z1_hat = tf.nn.batch_normalization(z1, batch_mean1, batch_var1,beta1, scale1,epsilon)\r\n",
        "    l1 = tf.nn.dropout(tf.nn.tanh(z1_hat), keep_probability)\r\n",
        "    \r\n",
        "    # Layer 2 with batch normalization\r\n",
        "    z2 = l1@weights['w2']\r\n",
        "    batch_mean2, batch_var2 = tf.nn.moments(z2,[0])\r\n",
        "    scale2 = tf.Variable(tf.ones([num_hidden_2]))\r\n",
        "    beta2 = tf.Variable(tf.zeros([num_hidden_2]))\r\n",
        "    z2_hat = tf.nn.batch_normalization(z2,batch_mean2,batch_var2,beta2,scale2,epsilon)\r\n",
        "    l2 = tf.nn.dropout(tf.nn.tanh(z2_hat), keep_probability)\r\n",
        "\r\n",
        "    # Layer 3 with batch normalization\r\n",
        "    z3 = l2@weights['w3']\r\n",
        "    batch_mean3, batch_var3 = tf.nn.moments(z3,[0])\r\n",
        "    scale3 = tf.Variable(tf.ones([num_hidden_3]))\r\n",
        "    beta3 = tf.Variable(tf.zeros([num_hidden_3]))\r\n",
        "    z3_hat = tf.nn.batch_normalization(z3,batch_mean3,batch_var3,beta3,scale3,epsilon)\r\n",
        "    l3 = tf.nn.dropout(tf.nn.tanh(z3_hat), keep_probability)\r\n",
        "\r\n",
        "    # Layer 4 with batch normalization\r\n",
        "    z4 = l3@weights['w4']\r\n",
        "    batch_mean4, batch_var4 = tf.nn.moments(z4,[0])\r\n",
        "    scale4 = tf.Variable(tf.ones([num_hidden_4]))\r\n",
        "    beta4 = tf.Variable(tf.zeros([num_hidden_4]))\r\n",
        "    z4_hat = tf.nn.batch_normalization(z4,batch_mean4,batch_var4,beta4,scale4,epsilon)\r\n",
        "    l4 = tf.nn.dropout(tf.nn.tanh(z4_hat), keep_probability)\r\n",
        "\r\n",
        "    # Layer 5 with batch normalization\r\n",
        "    z5 = l4@weights['w5']\r\n",
        "    batch_mean5, batch_var5 = tf.nn.moments(z5,[0])\r\n",
        "    scale5 = tf.Variable(tf.ones([num_hidden_5]))\r\n",
        "    beta5 = tf.Variable(tf.zeros([num_hidden_5]))\r\n",
        "    z5_hat = tf.nn.batch_normalization(z5,batch_mean5,batch_var5,beta5,scale5,epsilon)\r\n",
        "    l5 = tf.nn.dropout(tf.nn.tanh(z5_hat), keep_probability)\r\n",
        "    \r\n",
        "    # Output Layer\r\n",
        "    output = tf.nn.relu(tf.add(tf.matmul(l5, weights['wout']) , biases['bout']))\r\n",
        "    return output"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJPfPoDSZEEp"
      },
      "source": [
        "# Small batch size for faster convergence\r\n",
        "batch_size=4\r\n",
        "# Small epsilon value for the batch normalization\r\n",
        "epsilon = 1e-3"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-He4P8_ZPth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d30362f-a968-48b4-9d27-cf62c68b46ba"
      },
      "source": [
        "# train network\r\n",
        "\r\n",
        "prediction = fully_connected_net(x, keep_probability)\r\n",
        "cost = tf.compat.v1.losses.mean_squared_error(y,prediction, weights=1.0)\r\n",
        "train_step = tf.compat.v1.train.AdamOptimizer(learning_rate= 0.0001).minimize(cost)\r\n",
        "saver = tf.compat.v1.train.Saver()\r\n",
        "#feedforward and backpropagation\r\n",
        "epochs = 700\r\n",
        "sess = tf.compat.v1.Session(config=config)\r\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\r\n",
        "for epoch in range(epochs):\r\n",
        "    epoch_loss = 0\r\n",
        "    #start_index = 0\r\n",
        "    #X_shuffled, S_shuffled = shuffle(X_abs,S_abs)\r\n",
        "    random = np.arange(0, len(X_abs), batch_size)\r\n",
        "    np.random.shuffle(random)\r\n",
        "    for i in range(len(random)):\r\n",
        "        start_index = int(random[i])\r\n",
        "        end_index = int(start_index + batch_size)\r\n",
        "        if end_index > len(X_abs):\r\n",
        "            end_index = len(X_abs)\r\n",
        "        batch_x = np.array(X_abs[start_index: end_index]).swapaxes(1,2)\r\n",
        "        batch_y = np.array(S_abs[start_index: end_index]).swapaxes(1,2)\r\n",
        "        seqlen = np.array(X_len[start_index: end_index])\r\n",
        "        start_index = end_index + 1\r\n",
        "        _, err = sess.run([train_step, cost], feed_dict={x: batch_x, y: batch_y, seq_len: seqlen, keep_probability: 0.7})\r\n",
        "        epoch_loss += err\r\n",
        "    for j in range(epoch % 10 == 0):\r\n",
        "        print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\r\n",
        "print('Epoch ',epoch, ' completed out of ',epochs, 'loss: ', epoch_loss)\r\n",
        "saver.save(sess, 'model2/')\r\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  completed out of  700 loss:  7.0365296602249146\n",
            "Epoch  10  completed out of  700 loss:  6.470064997673035\n",
            "Epoch  20  completed out of  700 loss:  5.963576912879944\n",
            "Epoch  30  completed out of  700 loss:  5.4915430545806885\n",
            "Epoch  40  completed out of  700 loss:  5.0425132513046265\n",
            "Epoch  50  completed out of  700 loss:  4.649830460548401\n",
            "Epoch  60  completed out of  700 loss:  4.203336775302887\n",
            "Epoch  70  completed out of  700 loss:  3.864050030708313\n",
            "Epoch  80  completed out of  700 loss:  3.418289840221405\n",
            "Epoch  90  completed out of  700 loss:  2.9960726499557495\n",
            "Epoch  100  completed out of  700 loss:  2.5854645371437073\n",
            "Epoch  110  completed out of  700 loss:  2.230322629213333\n",
            "Epoch  120  completed out of  700 loss:  1.8529794216156006\n",
            "Epoch  130  completed out of  700 loss:  1.4862291812896729\n",
            "Epoch  140  completed out of  700 loss:  1.246391475200653\n",
            "Epoch  150  completed out of  700 loss:  0.9822478741407394\n",
            "Epoch  160  completed out of  700 loss:  0.8138556778430939\n",
            "Epoch  170  completed out of  700 loss:  0.6920242458581924\n",
            "Epoch  180  completed out of  700 loss:  0.5396752953529358\n",
            "Epoch  190  completed out of  700 loss:  0.4185364693403244\n",
            "Epoch  200  completed out of  700 loss:  0.37285932153463364\n",
            "Epoch  210  completed out of  700 loss:  0.3192143589258194\n",
            "Epoch  220  completed out of  700 loss:  0.25745822489261627\n",
            "Epoch  230  completed out of  700 loss:  0.2407906837761402\n",
            "Epoch  240  completed out of  700 loss:  0.208709504455328\n",
            "Epoch  250  completed out of  700 loss:  0.1765780933201313\n",
            "Epoch  260  completed out of  700 loss:  0.1398632861673832\n",
            "Epoch  270  completed out of  700 loss:  0.12837408669292927\n",
            "Epoch  280  completed out of  700 loss:  0.12153361365199089\n",
            "Epoch  290  completed out of  700 loss:  0.1028645746409893\n",
            "Epoch  300  completed out of  700 loss:  0.09897380135953426\n",
            "Epoch  310  completed out of  700 loss:  0.07996089104562998\n",
            "Epoch  320  completed out of  700 loss:  0.08168112672865391\n",
            "Epoch  330  completed out of  700 loss:  0.06934138108044863\n",
            "Epoch  340  completed out of  700 loss:  0.070760034956038\n",
            "Epoch  350  completed out of  700 loss:  0.05860328674316406\n",
            "Epoch  360  completed out of  700 loss:  0.04625415522605181\n",
            "Epoch  370  completed out of  700 loss:  0.04897589702159166\n",
            "Epoch  380  completed out of  700 loss:  0.04578308342024684\n",
            "Epoch  390  completed out of  700 loss:  0.04258032329380512\n",
            "Epoch  400  completed out of  700 loss:  0.033687453251332045\n",
            "Epoch  410  completed out of  700 loss:  0.03635928453877568\n",
            "Epoch  420  completed out of  700 loss:  0.03148157428950071\n",
            "Epoch  430  completed out of  700 loss:  0.03273735521361232\n",
            "Epoch  440  completed out of  700 loss:  0.034561372827738523\n",
            "Epoch  450  completed out of  700 loss:  0.02570223528891802\n",
            "Epoch  460  completed out of  700 loss:  0.03008335828781128\n",
            "Epoch  470  completed out of  700 loss:  0.028150924714282155\n",
            "Epoch  480  completed out of  700 loss:  0.013848137808963656\n",
            "Epoch  490  completed out of  700 loss:  0.01934677385725081\n",
            "Epoch  500  completed out of  700 loss:  0.013235404621809721\n",
            "Epoch  510  completed out of  700 loss:  0.01943480921909213\n",
            "Epoch  520  completed out of  700 loss:  0.013908545370213687\n",
            "Epoch  530  completed out of  700 loss:  0.01617295457981527\n",
            "Epoch  540  completed out of  700 loss:  0.013799960841424763\n",
            "Epoch  550  completed out of  700 loss:  0.010358402389101684\n",
            "Epoch  560  completed out of  700 loss:  0.009446649812161922\n",
            "Epoch  570  completed out of  700 loss:  0.013832025229930878\n",
            "Epoch  580  completed out of  700 loss:  0.01048807636834681\n",
            "Epoch  590  completed out of  700 loss:  0.014958782237954438\n",
            "Epoch  600  completed out of  700 loss:  0.006778776878491044\n",
            "Epoch  610  completed out of  700 loss:  0.010920712724328041\n",
            "Epoch  620  completed out of  700 loss:  0.010825201170518994\n",
            "Epoch  630  completed out of  700 loss:  0.006636648206040263\n",
            "Epoch  640  completed out of  700 loss:  0.009917888208292425\n",
            "Epoch  650  completed out of  700 loss:  0.008951689931564033\n",
            "Epoch  660  completed out of  700 loss:  0.00788683642167598\n",
            "Epoch  670  completed out of  700 loss:  0.009664500888902694\n",
            "Epoch  680  completed out of  700 loss:  0.005389326950535178\n",
            "Epoch  690  completed out of  700 loss:  0.0064171175472438335\n",
            "Epoch  699  completed out of  700 loss:  0.004775422115926631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model2/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq8vMQOTY5q_"
      },
      "source": [
        "# Import test data\r\n",
        "\r\n",
        "path_ns_te='/content/drive/MyDrive/Audio Enh. Dataset/babble_10dB/wav/'\r\n",
        "\r\n",
        "tex, TEX, TEX_abs, TEX_len = loadfile(path_ns_te, '_babble_sn10', flag=1)\r\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo0yUN1nVJAr"
      },
      "source": [
        "# Getting the predictions for test set\r\n",
        "\r\n",
        "for i in range(len(TEX_abs)):\r\n",
        "    #epoch_x = np.array(TEX_abs[0: len(TEX_abs)]).swapaxes(1,2)\r\n",
        "    epoch_x = np.zeros((1, TEX_abs[i].shape[1], TEX_abs[i].shape[0]))  \r\n",
        "    epoch_x[0,:,:] = TEX_abs[i].T\r\n",
        "    \r\n",
        "    TEM_pred= sess.run(prediction, feed_dict = {x: epoch_x, seq_len : np.array([TEX_len[i]]), keep_probability: 1.1})\r\n",
        "#print(TEM_pred[0][4])"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLnjQ01ePhf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yahdGcV9qHm"
      },
      "source": [
        "# Reconstruct the output files of the test data\r\n",
        "for i in range(len(TEX_abs)):\r\n",
        "  out1 = (TEM_pred[0,:TEX_len[i],:].T * TEX[i])\r\n",
        "  test1_recons = librosa.istft(out1, win_length= 1024, hop_length=512)\r\n",
        "  librosa.output.write_wav('/content/drive/MyDrive/Audio Enh. Dataset/Test_op_data/' + str(i+1) + '.wav', test1_recons, sr)    "
      ],
      "execution_count": 134,
      "outputs": []
    }
  ]
}